% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8

\chapter{IT-Specifications and Implementation}
\label{ch:it_specifications}
\epigraph{"What would life be if we had no courage to attempt anything?"}{- Vincent Van Gogh}


The previous two chapters focus on the theory of the prototype. In contrast, this chapter goes into detail of the implementation.

\section{The Prototypes Structure}

The structure of the prototype implementation is more or less identical to the five steps which are introduced in the requirements of the prototype (Chapter \ref{ch:requirements}). In Figure \ref{fig:51_Requirements_process_libs}, the steps are visualized again. Additionally, Figure \ref{fig:51_Requirements_process_libs} contains information about which tools are mainly used in each process step.

\begin{figure}[htp]
	\centering
	\fbox{\includegraphics[width=0.9\linewidth]{photo/51_Requirements_process_libs}}
	\caption{Python and the so-called Matplotlib library are used in the overall implementation of the prototype. Whereas Python is the primary programming language, Matplotlib is used to make visualisations. Also, it is used to create diagrams for this thesis to support statements like Figure \ref{fig:47_data-set-quantative} The data engineering, the preprocessing steps, the VGG16 model and the evaluation criteria are implemented with the help of the TensorFlow library. The SHAP library is used within the last step, to make transparent decisions.}
	\label{fig:51_Requirements_process_libs}
\end{figure}

Python and the so-called Matplotlib library are used in the overall implementation of the prototype. Whereas Python is the primary programming language, Matplotlib is used to make visualizations. For example, if a  dataset is successfully loaded (Section \ref{sec:input_data}), the images are visualized with Matplotlib. Furthermore, Matplotlib is used to create diagrams for the thesis in order to support statements as can be seen in the following listing:

\nopagebreak
\begin{python}[label={pred}, caption={Predictions of the pretrained VGG16 Architecture CNN model}]
	colors = ( '#ece7f2','#a6bddb','#2b8cbe')
	distincted_lables = np.array([2,10,100])
	fig, (ax1,ax2,ax3) = plt.subplots(1, 3,figsize=(15,10))
	ax1.bar(data_sets, data_sets_sizes, color=colors)
	ax1.set_title("Size (MB)")
	ax2.bar(data_sets, distincted_lables, color=colors)
	ax2.set_title("Distinced labels")
	ax3.bar(data_sets, (data_sets_sizes/distincted_lables), color=colors)
	ax3.set_title("Size / Distinced Label")
	plt.show()
\end{python}

The result of the code can be seen in Figure \ref{fig:47_data-set-quantative} and is used in the section about the input data for the prototype (Section \ref{sec:input_data}).

The data, the preprocessing steps, the VGG16 model and the evaluation criteria are implemented with the help of the TensorFlow library (Sections \ref{sec:input_data} - \ref{make_predictions}). The SHAP is used within the last step, to make transparent decisions (Section \ref{make_transparent_decisions}). 

In the following, it is described why the tools are used within the prototype implementation. Besides, the tools are compared to alternatives. 

\section{Python as programming Language for the prototype}

Python is the primary programming language for the prototype implementation because of the following reasons:

\begin{enumerate}
	\item Python is the most popular language when it comes to machine learning and deep learning. This can be observed in Figure \ref{fig:52_python_popular}. It shows the percentage of matching job postings and is created by the job portal Indeed. The Figure compares the languages Python, R, Java, Javascript, C, C++, Julia and Scala  \cite{LegacyCo5:online}. 
	
	\item Pythons excellent libraries, which allows implementing the required steps straight forward
	
	\item Because of its popularity and because the language is open source,  there is a vast community to help with problems. Furthermore, a lot of documentation for Python is available online.
	
	\item As already named as a reason above, Python offers a variety of libraries, and some of them are excellent visualisation tools as well. However, for the prototype implementation, it is necessary to be able to represent data in a human-readable format.
	
	\item Libraries like Matplotlib allow building plots for better data understanding and visualisation. Different application programming interfaces also simplify the visualisation process and make it easier to generate figures for transparent decisions.  
\end{enumerate}

A few alternatives to Python can be seen in Figure \ref{fig:52_python_popular}. Every programming language has its downsides, and none of these languages is perfect. For example, other programming languages are maybe faster and more efficient as Python. In comparison, Python is considered as the languages with better packages and the greater community. These two points there mentioned as reasons why Python is the choice for the prototype implementation. If the community and package support would increase in other languages, the decision could be different. 

\begin{figure}[htp]
	\centering
	\fbox{\includegraphics[width=0.7\linewidth]{photo/52_python_popular}}
	\caption{Python is the most popular language when it comes to machine learning and deep learning. The Figure shows the percentage of matching job postings and is created by the job portal Indeed. The Figure compares the languages Python, R, Jave, Javascript, C, C++, Julia and Scala \cite{LegacyCo5:online}.}
	\label{fig:52_python_popular}
\end{figure}

\begin{figure}[htp]
	\centering
	\fbox{\includegraphics[width=0.7\linewidth]{photo/53_boston_summary_plot_bar}}
	\caption{The Figure shows a build-in visualization from the SHAP library. It can be used to plot a visualization of the SHAP values for non-image data.}
	\label{fig:53_boston_summary_plot_bar}
\end{figure}

\section{Libraries within the Prototype implementation}

\subsection{Matplotlib}

Matplotlib is already mentioned in the section about the structure of the prototype as a tool for visualisation. Furthermore, it is said as one of the reasons why Python is used as the primary programming language in the first place. 

Matplotlib is quite easy. Usually, while plotting, the same structure is used in every new plot. For example, for questions that arose during this thesis, Matplotlib can be used to visualise the answers in an easy way. As can be seen in the following listing, the code which is used to create two different diagrams is quite similar:

\begin{python}[label={pred}, caption={Predictions of the pre-trained VGG16 Architecture CNN model}]
	# Figure 1
	
	colors = ( '#ece7f2','#a6bddb','#2b8cbe')
	distincted_lables = np.array([2,10,100])
	fig, (ax1,ax2,ax3) = plt.subplots(1, 3,figsize=(15,10))
	ax1.bar(data_sets, data_sets_sizes, color=colors)
	ax1.set_title("Size (MB)")
	ax2.bar(data_sets, distincted_lables, color=colors)
	ax2.set_title("Distinced labels")
	ax3.bar(data_sets, (data_sets_sizes/distincted_lables), color=colors)
	ax3.set_title("Size / Distinced Label")
	plt.show()
	
	# Figure 2
	
	labels = 'Train', 'Validation', 'Test',
	sizes = [80,10,10]
	explode = (0.1, 0, 0) 
	plt.rcParams['font.size'] = 20    
	fig1, ax1 = plt.subplots(figsize=(15,10))
	ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
	shadow=True, startangle=90, colors = colors)
	ax1.axis('equal') 
	plt.show()
\end{python}


The name comes from the fact that Matlab is emulated.  Matlab is an enterprise software environment and programming language which is traditionally used by scientists and companies to work on machine learning tasks. It is expensive, difficult to scale and as a programming language, it is slow in comparison with other programming languages which are introduced in Figure \ref{fig:52_python_popular}. However, Matlab offers excellent ways to implement data visualisation. So, Matplotlib in Python is used as a robust, free and easy library for data visualisation instead. 

Seaborn is an alternative approach than it comes to visualise data in Python. It is almost the same with libraries for visualisation and programming languages. Like described at the end of the python section, visualisation libraries have differences in use cases, scalability and many other things. So, based on this statement, the best visualisation tool for the particular example of the prototype implementation is Matplotlib. If the prototype would contain more statistics instead of image recognition, then Seaborn is the right choice because it has a lot of things suitable for mathematical tasks, built-in. The prototype has to deal with image data, and Matplotlib offers excellent opportunities to visualise them. For example, to get a first impression of the downloaded data, it takes just a few lines of code to visualise them: 

\begin{python}[label={pred}, caption={Predictions of the pre-trained VGG16 Architecture CNN model}]
	fig = plt.figure(figsize=(20, 4))
	
	for i in np.arange(20):
	ax = fig.add_subplot(2, 10, i+1, xticks=[], yticks=[])
	plt.imshow(x_train[:20][i], cmap='gray')
	ax.set_title(cats_vs_dogs_labels[y_train[:20][i].item()])
\end{python}

X\_train contains the image data and y\_train the corresponding labels as described in the Theory Chapter (Chapter \ref{ch:theory}).

\subsection{TensorFlow} 

TensorFlow is used as a full-stack library, for four out of five steps of the prototype implementation. That means it offers ways to implement data loading, data preprocessing, creating models and make predictions (Sections \ref{sec:input_data} - \ref{make_predictions}). 

TensorFlow offers different levels of abstraction. So, for the prototype implementation, a level of abstraction is used, which provides two things. First, a syntax which makes it easy to implement ideas and second enough freedom to adjust the code. To adjust the code is important because as described in the functional-specifications, the base of the pre-trained model has to be changed. If the syntax is overwhelming, it can make things worse. Mainly because the goal is to make transparent decisions, it is not good to use an abstraction level that confuses even more. This is why Keras, as a high-level application programming interface, is used as a sufficient level of abstraction. 

For example, if it comes to implement a VGG16 architecture as described in the functional specifics, this can help a lot. In the following listings, two things can be observed. First, the code which is used in the prototype implementation needs just a few lines of code to load a pre-trained VGG16 model. That means the code offers two things as well. At first, the model is written as executable code, and then the pre-adjusted weights get loaded:

\begin{python}[label={pred}, caption={Predictions of the pre-trained VGG16 Architecture CNN model}]
	from keras.applications.vgg16 import VGG16
	
	model = VGG16(weights='imagenet', include_top=True)
	model.summary()
\end{python}

The following listing shows an implementation from scratch, of a simple neural network with just one neuron:

\begin{python}[label={pred}, caption={Predictions of the pretrained VGG16 Architecture CNN model}]
	class NeuralNetwork:
	def __init__(self, x, y):
	self.input      = x
	self.weights1   = np.random.rand(self.input.shape[1],4) 
	self.weights2   = np.random.rand(4,1)                 
	self.y          = y
	self.output     = np.zeros(self.y.shape)
	
	def feedforward(self):
	self.layer1 = sigmoid(np.dot(self.input, self.weights1))
	self.output = sigmoid(np.dot(self.layer1, self.weights2))
	
	def backprop(self):
	# application of the chain rule to find derivative of the loss function with respect to weights2 and weights1
	d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))
	d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))
	
	# update the weights with the derivative (slope) of the loss function
	self.weights1 += d_weights1
	self.weights2 += d_weights2
\end{python}

It has more lines of code, even though the model is so simple. This points out the importance of a library than it comes to the implementation of a deep learning model.

Furthermore, the library is used to get the data and adjust it such it fits to the use case of my prototype. Instead of download the data by hand and use multiple libraries to get the result, TensorFlow provides a very handy end-to-end solution for the first two steps. With just one function call it downloads the images, scales them and split them into different parts:

\begin{python}[label={pred}, caption={Predictions of the pre-trained VGG16 Architecture CNN model}]
	(raw_train, raw_validation, raw_test), metadata = this.load(
	'cats_vs_dogs',
	split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
	with_info=True,
	as_supervised=True,
	)
\end{python}

Even the alternative library PyThorch is faster, and its syntax is even more apparent, the opportunities to handle two out of five steps with just one function call made the decision clear. TensorFlow is the best fit for the prototype implementation because it allows implementing the whole prototype with just a few lines of code. 

\subsection{SHAP Library for transparent decision}

Based on the work of Luneberg and Lee, the SHAP library offers a method which allows computing the SHAP values after each iteration from each layer.  For a visualization, as described in the requirements (Chapter \ref{ch:requirements}) and the functional specifics (Chapter \ref{ch:functional_specifications}), a separate implemented function is required. The SHAP software library others different classes and methods which optimise the calculation of SHAP values for various machine learning approaches. The prototype uses deep explainer, which is optimised for deep learning models. It is not only for convolutional neural networks, but at least, the implementation works fine and comes to a result in a sufficed amount of time. 

There is no approach which provides similar functionalities and classes as the SHAP Library. This is why the library can not be compared with others and has to be used for the calculations. Otherwise, the implementation has to be done from scratch. 

\subsection{Implementation}

Data visualisation and model explainability are two integral aspects of the prototype implementation. They are the binoculars helping to see the patterns in the data, which lead to prototypes decisions. That is why the prototype uses a function which combines the original image with another layer that shows the calculated values on top. The visualisation is based on the visualisation, which is already offered by the SHAP library for non-image data. Figure \ref{fig:53_boston_summary_plot_bar} shows such a kind of visualization from the SHAP library. 

It can just be implemented by calling the shap\_plot() Method. As a parameter, it gets the features, the calculated SHAP values and the corresponding machine learning model. The function which is implemented in the prototype works similar. However, it allows - by the following function call - to get visualizations of the SHAP values for image data:

\nopagebreak
\begin{python}[label={pred}, caption={Predictions of the pretrained VGG16 Architecture CNN model}]
	visualize_model_decisions(shap_values=shap_values, x=to_predict, 
	labels=index_names, figsize=(20, 40))
\end{python}

The output of the function can be seen in Figure \ref{fig:41_expected_gradients_layer_7}. More precisely, the images show how the function was used on the dataset after predictions were made.

\begin{figure}[htp]
	\centering
	\fbox{\includegraphics[width=0.9\linewidth]{photo/41_expected_gradients_layer_7}}
	\caption{The output of the visulaize\_model\_deciions function. More precisely, the images show how the function was used on the dataset after predictions were made to visualize the SHAP values of the 7th layer in a convolutional neural network.}
	\label{fig:41_expected_gradients_layer_7}
\end{figure}

Note that not every aspect of the shown listings is explained in any detail. They provide a better understanding of the mentioned points in each section. The theoretical background and the functional usage towards the listings is described in the corresponding section at the Functional Specifications (Chapter \ref{ch:functional_specifications}) and \hyperref[ch:theory]{Theory Chapter (\ref{ch:theory})}. 