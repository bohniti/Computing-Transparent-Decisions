% !TeX spellcheck = en_GB
% !TeX encoding = UTF-8

\chapter{Generalization}
\label{ch:generalization}
\epigraph{"The big picture doesn't just come from distance; it also comes from time."}{- Simon Sinek}

The prototype was successful due to the evaluation process within the last section. It can be seen how the decisions of an image recognition task can be visualized. This chapter focus on the question of what this means in a more general sense. 

The Introduction presented a couple of reasons why explainable artificial intelligence is more and more critical in the next few years, especially when it comes to the use of upload filters in social media. 

In the following, this statement will be combined with the evaluated results of each category to draw a complete picture:

\section{Ability to predict}

As can be seen, it is possible to recognize patterns on images. The problem is that real-world use cases are not binary as the prototype assumes (cats and dogs). Moreover, it is a real challenge to get the right amount of suitable training examples. Research points out that there is still a lot of work to do if it comes to the use of convolutional neural networks in praxis.

At the same time, research points out that convolutional neural networks are used in praxis and that around 90\% is enough accuracy if the application is not critical as in social media. Besides, even then it is crucial to have a high accuracy as in the financial sector, those networks already used in practice. 

The statement above leads to the assumption that it is still essential to have explainable models, as shown in the Introduction, even though it will take a few more years as those networks become best practice. 

\section{Ability to predict Shapley values}

One of the drawbacks of SHAP values is its calculation time. Because of the popularity of mobile devices, it gets more and more essential to use efficient methods. This is also true when it comes to calculating transparent models, for example, for the use within an upload filter. So, in general, this leads to the statement that SHAP values are not very good, then it comes to upload filters because it needs vast hardware resources and takes long for computation. 

It would be better to have methods which can be computed within seconds such that users had a transparent decision as described in the Introduction immediately. 

There are a few methods which are already fast in comparison with the SHAP values. They could be the right choice in the upcoming future.

\section{Ability to visualize SHAP values}

Figure \ref{fig:41_expected_gradients_layer_7} shows that it is possible to visualize SHAP values to make a model more transparent. It is at least questionable if a non-technical user can understand the intuition about it. 

The discrepancy between technical and non-technical users is typical for explainable artificial intelligence. A user who is in the field in computer science or something related to it can understand visualization of SHAP values. Still, it is hard to say if the visualization is transparent enough for every user. For example, it is not clear if an Instagram user will have any idea why its contend get blocked after he or she had a look on the SHAP value visualization. 

I think in the upcoming future visualizations are the key to success then it comes to explainable artificial intelligence. The problem is that this is not just a field of machine learning engineers and data scientists. Instead, it is a cross-functional filed for user experience designers, business analysts, computer scientists and machine learning engineers. For example, for a transparent decision of an upload filter, it is essential to understand the interaction between the user and the visualization (user experience design). Furthermore, the model shall be in the interest of the companies interests (business analysts), and the ideas must be computable with as few resources as possible (computer scientists). Last but not least, the visualization shall present the correct results (data scientist). 
