% !TeX spellcheck = en_GB
\chapter{Introduction}
\label{ch:Introduction}
\epigraph{"Most times, the way isn’t clear, but you want to start anyway. It is in starting with the first step that other steps become clearer."}{― Israelmore Ayivor, Leaders' Frontpage: Leadership Insights from 21 Martin Luther King Jr. Thoughts}
\section{Upload Filter in the public discussion}
\label{sec:directiveCopyrightUploadFilters}

The "Directive on Copyright in the EU Digital Single Market", which came into force on the 7th of June in 2019,  introduced a requirement for an automated upload filter. The discussion refers to Article 17 (previously Article 13) because it forces content-sharing providers such as Facebook, Google and YouTube to be responsible for copyright violations. From this point on, it is no longer possible to check contents manually \cite{Emmawoollacott2019}.\\

\hyperref[fig:00_protest_gegen_upload_filter]{Figure \ref{fig:00_protest_gegen_upload_filter}} shows protesters in Berlin. For them, the fear of an upload filter is real. They are concerned that these filters can harm their business as small content creators or their freedom of speech.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.9\linewidth]{photo/00_protest_gegen_upload_filter.jpeg}
	\caption{In Berlin, opponents of the copyright directive are protesting. They fear that Article 13 will lead to upload filters, which they see as a danger considering different aspects \cite{Protestg81:online}.}
	\label{fig:00_protest_gegen_upload_filter}
\end{figure}

Different parties, organisations and scientists are arguing for and against the copyright directive. For example, the Christian Democratic Union (CDU) and its representative at the European Parliament Axel Foss support the directive.  He said that it protects the freedom of expression on the internet and a diverse media landscape \cite{Europarl2017}. Critics as Julia Reda from the Pirate Party  (also a member of the European Parliament) denies this statement. She described it as a change into a dark future for internet freedom, when upload filters would check every content automatically \cite{Emmawoollacott2019}.\\

More precisely, she assumes that technology does not know the nuances of particular copyright law yet. And even more, she fears that small content creators are not able to interpret the decisions of the upload filters \cite{Reda2019}. Dr Gallwitz is a  professor at the Technical University of Nuremberg where he is researching in the field of pattern recognition. He criticised that there is no smart software that intelligently recognises patterns \cite{Gallwitz2019}.\\

Instead of using smart software, a real-world upload filter is realised through fingerprinting. This technique allows comparing blocked content with the latest user-generated content. The hardware requirements which are needed to examine the contents are proportionally small. Fewer hardware requirements and a lack of transparency and reliability are reasons why most companies still use older techniques (e.g. hash values), even they can develop smart software for upload filters \cite{Spoerri2019} \cite{Wagner1983}.\\

The Microsoft software Photo-DNA identifies child pornography with the fingerprinting method, e.g. if an image in Microsoft's database is marked as pornographic. That means YouTube can detect when someone wants to upload an image to the platform which contains offensive content \cite{Microsoft2013}. The Software Content ID uses the same technique to identify copyright offences. It is used and developed by YouTube. That shows how technology companies are responsible for the methods of choice when it comes to uploading filters. Even though scientific research has shown that different approaches are performing well, the companies have to see this decision from a business point of view \cite{YouTube2010}.\\

Using deep learning models as upload filters in the future could be possible, but among this usage are many problems. One of them is to get labelled data. Therefore, labelled data is necessary to train a machine learning model, e.g. which can detect patterns from images. Another problem is to define the patterns or the labels itself \cite{WaltermannHubertus2019}. Labels are categories that are assigned to an image \cite{Goodfellow-et-al-2016} e. g. attach the label cat to an image with a cat. In the case of copyright violations or offensive content, any item that has labels with copyright content will be blocked. A machine learning algorithm which can detect every offensive content must be very complex because the complexity from machine learning algorithms is increasing with the number of growing input features and an increasing number of possible labels \cite{Yao2017}. In machine learning, a feature can be any input which can feed into a machine learning algorithm. For example, an image with 32 x 32 pixels has 1024 input features because every pixel is a feature \cite{Goodfellow-et-al-2016}.

\section{Upload filters and Explainable Artificial Intelligence}
\label{sec:uploadFiltersXAI}

As mentioned in the introduction to the Directive on EU Copyright and Upload Filters (\hyperref[sec:directiveCopyrightUploadFilters]{Section \ref{sec:directiveCopyrightUploadFilters}}), a machine learning algorithm, which is used to detect copyright infringements would be very complex. So, it is not often used in practice right now. Joel Dudley, director of the Institute for Next-Generation Healthcare at the Ichan School of Medicine, said: "We can build these models, but we don't know how they work \cite{Knight2019}. His statement was part of an interview with the journalist Will Knight from MIT's journal Technology Review about explainable artificial intelligence. A precise definition of explainable artificial intelligence does not exist. The idea is that we use a machine learning algorithm to make decisions which must be in natural language. For example, if we use an image as an input for a machine learning algorithm, the task would be to label the images with a label for cancer or no cancer. A doctor who wants to use this information could be interested in the question of why the algorithm predicts a cretin label for this image \cite{SamekWojciech2017}.\\

Apart from the Technology Review, newspapers like The New York Times, Financial Times and The Register have also reported that an explainable artificial intelligence is necessary when a machine learning algorithm is used for automatic decision making \cite{Kuang2017} \cite{Robinson2017} \cite{Waters2017}. David Gunning explains in his research project about explainable artificial intelligence that machine learning can bring a huge benefit to the transportation sector, as well as to the finance, security, legal, medicine and military sector. Gunning claims that algorithms are limited because they can not explain their actions and decisions to users in an understandable way. He assumes explainable artificial intelligence will be essential if users want to understand, trust, and effectively manage machine learning algorithms in their applications \cite{Gunning2019}. \hyperref[fig:01_upload_filiter_example]{Figure \ref{fig:01_upload_filiter_example}} shows an example of how transparent upload filters can be applied to uploaded images. When we build an upload filter with deep learning methods, it has to be an explainable artificial intelligence \cite{WaltermannHubertus2019}. Otherwise, there will be no explanation of how the decision from an upload filter was made.\\

\begin{figure}[!htp]
	\centering
	\includegraphics[width=1\linewidth]{photo/01_upload_filiter_example.png}
	\caption{Example of how explainable artificial intelligence can be applied to uploaded images \cite{Visualiz76:online}}
	\label{fig:01_upload_filiter_example}
\end{figure}

\section{The Objective of this Thesis}
\label{sec:goal}

Even though automatic image recognition with an explainable artificial intelligence is just a research topic, it is important to evaluate how to generate transparent decisions (XAI). So, the main objective of this thesis is to find an answer to the following question:
\begin{quote}\textit{"How can automatic image recognition be implemented in such a way that the resulting decisions are transparent?"} \end{quote}
For this purpose, a prototype will be developed, which is able to recognise patterns on images (automatic image recognition) automatically. If a pattern is recognised by the algorithm, it assigns a label to the image.  In the pre-field,  it gets defined which patterns shall recognised by the deep learning algorithm. 

Afterwards, the results of the algorithm are presented in such a way that the decisions become transparent (explainable artificial intelligence). What transparent actually means is hard to tell, because it depends on the algorithms and techniques, which are used to implement the filter. Anyway, for this thesis, transparent shall be defined as follows:

\begin{quote}
	\textit{Imagine a social media platform like Instagram, where everybody shares its images. If someone shares an image and it is getting blocked by a filter (e.g. because of copyright violation), the user should be able to understand why the decision was made. The explanation can be an automatically generated text, which says that the algorithm had detected a copyright issue and it highlights specific parts in the blocked image. The highlighted details tell the user why the algorithm came to a decision.
	}
	
\end{quote}

In contrast to my example, a real-world application is much more complicated. First, in a real-world application, has more labels. Second, it is not only possible to upload images. Instead, also text and sound have to be checked. This is why the application is only a prototype. Anyway, the research question will be answered anyways at least from a theoretical perspective. In the end, the prototype is used to transfer the insights gained through the prototype to a more general scenario.

\section{Thesis Structure}
\label{sec:structure}

The Introductions objective is to present context information around the topic and to resent the research question, the structure and related work (\hyperref[ch:Introduction]{Chapter \ref{ch:Introduction}}). While the terms were already briefly introduced in the introduction the Background and Theories chapter objective to present the terms and the functions in concern to the research question  (\hyperref[ch:theory]{Chapter \ref{ch:theory}}). The Requirements chapter is a description of the possible scope of the prototype and a collection of the functional requirements for the prototype (\hyperref[ch:requirements]{Chapter \ref{ch:requirements}}). The Functional Specifications chapter aims to present set of functional specifications which can be used to decide how the technical side of the implementation will be done (\hyperref[ch:functional_specifications]{Chapter \ref{ch:functional_specifications}}). The IT-Specifications shall present a collection of instruments for the prototypes implementation which will be used together with the functional specification set to implement the prototype (\hyperref[ch:it_specifications]{Chapter \ref{ch:it_specifications}}). The evaluation shall present is an overview of the number of fulfilled and unfulfilled requirements of the prototype (\hyperref[ch:summary]{Chapter \ref{ch:summary}}). The Generalization chapter is an analysis of the obtained evaluation results in a more general context  (\hyperref[ch:generalization]{Chapter \ref{ch:generalization}}). The Conclusion is a personal statement regarding the results of the evaluation (\hyperref[ch:conclusion]{Chapter \ref{ch:conclusion}}).

The Methodological Table (\hyperref[fig:02_methodological_table]{Figure \ref{fig:02_methodological_table}})  also presents the thesis structure. It was prepared to get an overview of the topic and was created with the help of Prof. Dr. Alfred Holl. It was beneficial to bring the questions of interest in a structured and logical order. 

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.8\linewidth]{photo/02_methodological_table.pdf}
	\caption{Methodological table  which was prepared in the run-up to the bachelor thesis in cooperation with the assesor Dr Prof Alfred Holl.}
	\label{fig:02_methodological_table}
\end{figure}

\section{Related Work}

Katharina Blandina Weitz master thesis "Applying Explainable Artificial Intelligence for Deep Learning Networks to Decode Facial Expressions of Pain and Emotions" is about programmes used in hospitals, which detect pain in facial expressions. According to her, this software should use deep learning to support human practitioners. In her thesis, Ms Weitz finds out that these deep learning methods work well in recognising pain in human facial expressions, but they are hard to interpret. From her abstract, it can be seen that explainable artificial intelligence does not depend on specific data sets. Rather, according to her statement, the methods can be applied to any dataset that was previously was processed by deep learning before\cite{Weitz2018}. As described in the corresponding section the contexted will be changed in this thesis (\hyperref[sec:goal]{Section \ref{sec:goal}}).\\

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.6\linewidth]{photo/03_DARPA_XAI_Project.png}
	\caption{Explainable artificial intelligence concept of the Defense Advanced Research Projects Agency \cite{Robinson2017}}
	\label{fig:03_DARPA_XAI_Project}
\end{figure}

The Defence Advanced Research Projects Agency is an agency of the United States Department of Defence. They focus on the development of emerging technologies that the military can use in the future. Because both the number of applications and the complexity of machine learning models have increased, a project has been started to specialise in explainable artificial intelligence. It aims for more transparent models while maintaining a high level of prediction accuracy. Furthermore, the program wants to enable human users to understand, appropriately trust, effectively manage the output of a machine learning algorithm  \cite{Robinson2017}. The Defense Advanced Research Projects Agency presents its projects as a concept (\hyperref[fig:03_DARPA_XAI_Project]{Figure \ref{fig:03_DARPA_XAI_Project}}). The concept aims to move away from models which human users cannot understand towards more transparent models. A step towards to transparent models is also the main objective of the prototype implementation (\hyperref[sec:goal]{Section \ref{sec:goal}}).

Of course more scientific papers are published on explainable artificial intelligence, but to present a complete bibliography is not the objective (\hyperref[sec:goal]{Section \ref{sec:goal}}).\\
